{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForMaskedLM\n",
    "from splade_preprocessor import SparseDocTextPreprocessor\n",
    "from pymongo import MongoClient\n",
    "import torch.autograd.profiler as profiler\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "model_names = [\n",
    "    \"naver/splade_v2_max\",\n",
    "    \"naver/splade_v2_distil\",\n",
    "    \"naver/splade-cocondenser-ensembledistil\",\n",
    "    \"naver/efficient-splade-VI-BT-large-query\",\n",
    "    \"naver/efficient-splade-VI-BT-large-doc\",\n",
    "]\n",
    "splade=  model_names[3]\n",
    "def tokenizer2(func):\n",
    "    def _tokenizer2(*args, **kwargs):\n",
    "        to_return = func(*args, **kwargs)\n",
    "        del to_return['token_type_ids']\n",
    "        return to_return\n",
    "    return partial(_tokenizer2, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=128)\n",
    "\n",
    "doc_text = MongoClient('localhost',27017)['catalogStore']['doc_text']\n",
    "doc_samples = []\n",
    "counter = 0\n",
    "for doc in doc_text.find({}).limit(32):\n",
    "    counter += 1\n",
    "    doc_samples.append(doc['text'])\n",
    "    if counter == 32:\n",
    "        break\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(splade)\n",
    "# spl_tokenizer = tokenizer2(tokenizer)\n",
    "spl_tokenizer = partial(tokenizer, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "# spl_tokenizer(\"hell\")\n",
    "proc = SparseDocTextPreprocessor()\n",
    "doc_samples = [proc.clean_text(doc) for doc in doc_samples]\n",
    "tokens = spl_tokenizer(doc_samples, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens['input_ids']\n",
    "token_type_ids = tokens[\"token_type_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/op3ntrap/Envs/miniforge/envs/splade/lib/python3.10/site-packages/torch/onnx/utils.py:847: UserWarning: no signature found for <torch.ScriptMethod object at 0x7eb141366c00>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "# from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor\n",
    "\n",
    "\n",
    "class TransformerMLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # with profiler.record_function(\"model init\"):\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(splade, torchscript = True)\n",
    "        # self.model.to('cuda')  # type: ignore\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # with profiler.record_function(\"model forward\"):\n",
    "        with torch.cuda.amp.autocast(enabled=True):  # type: ignore\n",
    "            with torch.no_grad():\n",
    "                # This model produces a tuple as an output\n",
    "                return self.model(input_ids, token_type_ids, attention_mask)[0]\n",
    "                \n",
    "\n",
    "    \n",
    "class SparseModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bertlm = TransformerMLM()\n",
    "        self.bertlm.eval()\n",
    "        # self.bertlm = self.bertlm.to(\"cuda\")\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        with torch.cuda.amp.autocast(enabled=True):  # type: ignore\n",
    "            with torch.no_grad():\n",
    "                mlm_logits = self.bertlm(input_ids, token_type_ids, attention_mask)\n",
    "                mlm_logits, _ = torch.max(\n",
    "                    torch.log(1+torch.relu(mlm_logits))*attention_mask.unsqueeze(-1),\n",
    "                dim=1\n",
    "                )\n",
    "                del _\n",
    "                return mlm_logits         \n",
    "\n",
    "\n",
    "sm = SparseModel()\n",
    "# sm = sm.to(\"cuda\")\n",
    "sm = sm.eval()\n",
    "traced_model = torch.jit.trace(sm, [input_ids,token_type_ids,attention_mask])\n",
    "torch_jit_model_path = splade.replace(\"naver/\",\"splade_models/\")+'.pt'\n",
    "onnx_model_path = splade.replace(\"naver/\",\"splade_onnx/\")+'.onnx'\n",
    "traced_model.save(torch_jit_model_path) # type: ignore\n",
    "del sm \n",
    "sm = torch.jit.load(torch_jit_model_path)\n",
    "# sm = sm.to(\"cuda\")\n",
    "sm = sm.eval()\n",
    "torch.onnx.export(\n",
    "    sm,\n",
    "    (input_ids,token_type_ids,attention_mask),\n",
    "    onnx_model_path,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input_ids\",\"token_type_ids\",\"attention_mask\"],\n",
    "    output_names=[\"sparse_embeddings\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  # variable lenght axes\n",
    "        \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  # variable lenght axes\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"sparse_embeddings\":{0: \"batch_size\"}\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = sm(input_ids,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = mlm.nonzero(as_tuple=True)\n",
    "a = mlm.nonzero()[:,0]\n",
    "b = mlm.nonzero()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4475, 0.2231, 0.3381,  ..., 0.4599, 0.1003, 0.0896], device='cuda:0',\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpareseResults(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "\n",
    "    def forward(self,mlm_logits):\n",
    "        with torch.no_grad():\n",
    "            batch_size = mlm_logits.size(0)\n",
    "            mlm_nz = mlm_logits.nonzero()\n",
    "            vec_indices = torch.vstack((mlm_nz[:,0], mlm_nz[:,1]))\n",
    "            vec_values = mlm_logits[mlm_nz[:,0], mlm_nz[:,1]]\n",
    "            del mlm_logits\n",
    "            results = torch.zeros((batch_size, 2,  512),device=\"cuda\")\n",
    "            for row in range(batch_size):\n",
    "                indices = torch.zeros(512,device=\"cuda\")  # type: ignore\n",
    "                values = torch.zeros(512,device=\"cuda\")  # type: ignore\n",
    "                mask = vec_indices[0].eq(row)\n",
    "                row_indices = torch.masked_select(vec_indices[1], mask)\n",
    "                indices[:row_indices.shape[0]] = row_indices\n",
    "                row_values = torch.masked_select(vec_values, mask)\n",
    "                values[:row_values.shape[0]] = row_values\n",
    "                result = torch.vstack((indices, values))\n",
    "                results[row] = result\n",
    "            return results\n",
    "\n",
    "sp = SpareseResults()\n",
    "sp = sp.to(\"cuda\")\n",
    "res = sp(mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = torch.jit.script(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.compile(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.save(\"splade_models/sparse.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = spl_tokenizer(doc_samples, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens['input_ids'].to(\"cuda\")\n",
    "attention_mask = tokens[\"attention_mask\"].to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 ms, sys: 1.09 ms, total: 22.1 ms\n",
      "Wall time: 21.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "indices = sm(input_ids = input_ids, attention_mask=attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = splade.replace(\"naver/\",\"splade_onnx/\")+'.onnx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/op3ntrap/Envs/miniforge/envs/splade/lib/python3.10/site-packages/torch/onnx/utils.py:847: UserWarning: no signature found for <torch.ScriptMethod object at 0x79916054b470>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n",
      "/home/op3ntrap/Envs/miniforge/envs/splade/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5859: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-02-12 16:25:02.220570132 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 257 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-02-12 16:25:02.225388700 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-02-12 16:25:02.225393861 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CUDAExecutionProvider\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg at 0x70e2f5f91d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"splade_onnx/splade_model_coco_ensemb_opt.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = {k:tokens[k].numpy() for k in tokens}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
